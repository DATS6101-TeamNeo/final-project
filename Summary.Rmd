---
title: "Summary Paper: Effect of Major Historical Events on Sectors of S&P500 Index"
author: "Bharat Khandewal, Dinesh Chandra Gaddam, Phanindra Kumar Kalaga, Prudhvi chekuri"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    theme: cayman
    toc: yes
---


```{r init, include=FALSE}
# Load all required packagess
library(xts)
library(rvest)
library(dplyr)
library(ezids)
library(ggplot2)
library(tseries)
library(corrplot)
library(quantmod)
library(forecast)
library(lubridate)
library(tidyverse)

knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
```

## 1.Abstract
<span style="  text-align: justify;">
<p> In this project, we aim to analyze the impact of major historical events on aggregate of the companies belonging to same sector of the S&P 500 index, focusing on the Great Recession, the COVID-19 pandemic, and the Russia-Ukraine conflict. By examining the index's performance before and after these events, we seek to understand how economic downturns, geopolitical crises, and global pandemics influence stock market behavior.

<P>The objective of this project is to explore the relationship between the companies belonging to same sector of the S&P 500 index and these major historical events, providing insights into the index's resilience, volatility, and performance during times of crisis. By analyzing the index's behavior during these critical periods, we aim to uncover patterns, trends, and correlations useful for data-driven decision-making.

<p> Our analysis will focus on key metrics such as volatility, returns, and performance indicators, comparing the companies belonging to same sector of the S&P 500 index's performance during the Great Recession, COVID-19 pandemic, and Russia-Ukraine conflict. By examining these three distinct periods, we aim to identify how the index responds to external shocks, recovers from downturns, and adapts to changing market conditions.

<p> The results of this analysis can provide valuable insights for investors, financial analysts, and policymakers, helping them navigate turbulent market conditions, mitigate risks, and capitalize on opportunities. By understanding the impact of major historical events on the sectors of S&P 500 index, stakeholders can make more informed decisions, optimize their investment strategies, and achieve better outcomes in the stock market.
</span>

## 2. Introduction
<span style=" text-align: justify;">
<p>
The S&P 500 is a stock market index that measures the performance of 500 large-cap U.S. companies listed on stock exchanges. An index in finance refers to a statistical measure of change in a securities market. It typically represents a portfolio of securities that represent a particular market or sector. Stock market indices are used to track the performance of a specific group of stocks.overall, The index is market-capitalization -weighted, meaning that larger companies have a greater impact on its value. Now, coming to the S&P 500, it includes companies from diverse industries such as technology, healthcare, financial services, consumer discretionary and industries. The S&P 500 is widely regarded as one of the best benchmarks for the performance of the U.S. stock market.
</P>
<P>This project utilizes a dataset from Yahoo finance containing information of all companies stock prices from 2000 to 2024, with the goal of developing Explanatory Data Analysis to effectively distinguish Data observations for analysis. The dataset includes characteristics such as symbol(symbol of company), open, close, High, Volume,GICS sectors and Low. Through exploratory data analysis, we will identify trends and correlations between these features and gather insights. Firstly we will see if the dataset needs any changes like pre processing then we will continue with analysis using t-test's. 
</P>
<P>The results produced from this project  can provide valuable insights into market dynamics, risk management, investment strategies, and decision-making, contributing to more informed and successful investing over time.
</P>
</span>

## 3. Dataset 
<span style=" text-align: justify;">
<p>Our dataset is available as [S&P 500 (^GSPC)](https://finance.yahoo.com/quote/%5EGSPC/history/?guccounter=1) on Yahoo Finance website. All the data is sourced from Yahoo Finance website and covers data from 2003-2023. The dataset consists of 5285 data points (records) and 6 features(columns). Also, there are no null values in the dataset.
<P>
<p>Featuresâ€™ description is given below;
</span>

<div class="column-definitions">
  <p><strong>1. Symbol:</strong>The stock symbol of the company in the S&P500 Index.</p>
  <p><strong>2. Open:</strong> Opening price of the S&P 500 index on a given date.</p>
  <p><strong>3. Close:</strong> Closing price of the S&P 500 index on a given date.</p>
  <p><strong>4. High:</strong> The highest price
  attained by S&P 500 index on a given date.</p>
  <p><strong>5. Low:</strong> The lowest price observed for the S&P 500 index on a given date.</p>
  <p><strong>6. Volume:</strong> The total number of shares of all S&P 500 index components traded on a given date.</p>
  <p><strong>7. Daily Change:</strong> The difference in price between the current day's closing price and the previous day's closing price.</p>
  <p><strong>8. GICS.Sector:</strong> The Global Industry Classification Standard (GICS) sector of the company.</p>
</div>

<style>
  .column-definitions {
    font-family: Arial, sans-serif;
    padding: 20px;
    border: 1px solid #ccc;
    background-color: #f7f7f7;
  }
</style>

</br>
+ "symbol" - The stock symbol of the company in the S&P500 Index.
+ "date" - The date of the stock price.
+ "open" - The opening price for the specified date(s).
+ "high" - The high price for the specified date(s).
+ "low" - The low price for the specified date(s).
+ "close" - The closing price for the specified date(s).
+ "adjusted" - The adjusted closing price for the specified date(s).
+ "volume" - The volume for the specified date(s).
+ "GICS.Sector" - The Global Industry Classification Standard (GICS) sector of the company.

<p>Our research focuses on analyzing how major historical events have influenced the behavior of the S&P 500 index using constituent data spanning the last 23 years, from 2000 to 2024. Our analysis is multifaceted, aiming to provide a comprehensive understanding of these impacts:

<p>Evolution of Sector Composition: We will analyze the number of companies in each sector and how this composition has changed over time. This analysis will shed light on sectoral trends and shifts in the S&P 500 index constituents.
<p>Sector Contributions to Index Performance: Through visualization and analysis, we seek to identify which GICS sectors have been primary drivers of the S&P 500 index. Understanding these key sectors can provide insights into the index's overall performance.
<p>Impact of Events on Industries: We will examine how specific industries within the index respond to major historical events. By evaluating their reactions and how they influence average price levels, we aim to discern patterns and correlations.
<p>T-tests on Sector Closing Prices: To quantitatively assess the impact of events, we will conduct T-tests to compare the average closing prices of sectors before and after these events. This statistical analysis will help us understand if there are significant differences in sector performance.
<p>Forecasting the S&P 500 Index: Using ARIMA and LSTM models, we will forecast the S&P 500 index to gauge the effectiveness of these models in predicting its future values. This analysis will provide insights into the predictability of the index.
<p>Model Evaluation: We will evaluate the performance of the ARIMA and LSTM models using MAE, MSE, and RMSE. These metrics will help us assess the accuracy and reliability of the forecasting models.

### Step 1: Loading the dataset
```{r}
datasets.url <- "https://drive.usercontent.google.com/download?id=1BqKbN3FEqNcRXeisOPxR_LB8VcSCh7mE&export=download&authuser=0&confirm=t"
#df <- read.csv("sp500.csv")
df <- read.csv(datasets.url)
df$Date <- as.Date(df$Date)
str(df)
```


### Step 2: Data Preprocessing

### Checking for NA values and duplicates
```{r}
# Checking for missing values
sum(is.na(df))
```


<p>Our dataset, spanning the last 23 years from 2000 to 2024, is remarkably clean, with no missing values (r sum(is.na(df))). This indicates that our data is complete and does not require any imputation or removal of data points. This high data quality enhances the reliability and validity of our analyses, allowing us to draw meaningful insights into the behavior of the S&P 500 index in response to major historical events.


### Dropping unwanted variables extracting numerical variables

```{r}
# Making GICS code as a factor variable
df$GICS <- as.factor(df$GICS)
# Removing the adjusted column as it is not required for our analysis
newdf <- df %>% select(-c("Adj.Close")) %>%
  arrange(Date) %>% # sorting the data by date
  mutate(daily_returns = (Close - lag(Close)) / lag(Close))

# remove first row to avoid NA values
newdf <- newdf[-1,]
head(newdf)
str(newdf)
```
<span style="  text-align: justify;">

<p>In the preprocessing stage of our analysis, we've streamlined our dataset by removing the "Adj.Close" column, which isn't pertinent to our current objectives. We've also organized the data chronologically by date and calculated the daily returns for each entry, essential for our analysis of trends and patterns. Additionally, to enhance our analytical capabilities, we've converted the GICS column into a factor variable. This transformation allows for more effective categorization and exploration of the dataset, considering the r nlevels(newdf$GICS) distinct levels within the GICS classification. These steps lay a solid foundation for our subsequent analyses and modeling efforts.

```{r}
# Define the event windows
great_recession_start <- as.Date("2007-12-31")
great_recession_end <- as.Date("2009-06-30")

covid_start <- as.Date("2020-02-03")
lockdown_end <- as.Date("2020-08-31")
covid_end <- as.Date("2023-05-11")

russia_ukraine_start <- as.Date("2022-02-24")
russia_ukraine_end <- Sys.Date()  # Current date

# The Great Recession
df_recession <- df[df$Date >= "2007-12-31" & df$Date <= "2009-06-30", ]

# Covid-19
df_covid <- df[df$Date >= "2020-02-03" & df$Date <= "2023-05-11", ]

# Russia-Ukraine Invasion
df_ukraine <- df[df$Date >= "2022-02-24", ]
```



### Checking if unique Symbols in the dataset are 500

<p> Our dataset comprises r length(unique(newdf$Symbol)) unique symbols, each representing a distinct entity. These symbols are associated with r head(unique(newdf$GICS)) unique sectors, reflecting the diverse industries or segments represented in our data. Understanding the sectors can provide valuable context for our analysis. Additionally, exploring the unique symbols, such as r head(unique(df$Symbol)), can offer insights into the specific companies or entities we're examining, potentially revealing trends or patterns specific to these entities.

### Plot to show number of companies in each GICS sector 
```{r}
industry_counts <- df %>%
  distinct(Symbol, GICS) %>%
  count(GICS)

ggplot(industry_counts, aes(x = GICS, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Companies per Industry", x = "Industry (GICS)", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

<p>The bar chart provides a comprehensive overview of the distribution of businesses across different industries, categorized according to the Global Industry Classification Standard (GICS). It illustrates that the "Industrials" sector, encompassing manufacturing companies, boasts the highest number of businesses among all industries. Following closely behind are the "Financials" sector, comprising financial services firms, and the "Information Technology" sector, which includes technology and software companies. This distribution of businesses across industries sheds light on the relative prominence and scale of different sectors within the dataset, offering valuable insights into the composition and structure of the market or dataset being analyzed. Such insights can be instrumental in strategic decision-making, investment analysis, and market research, among other applications.

### Stacked Bar Plot to show how S&P500 Companies in the GICS Sectors have changed over time
```{r}
ggplot(df, aes(x = year(df$Date), fill = GICS)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

<p>The analysis indicates that the distribution of businesses among various industries is notably concentrated in four sectors: Financials, Information Technology, Health Care, and Industrials. This concentration suggests that these sectors play a significant role in the overall business landscape, potentially influencing market trends and economic dynamics. Understanding the dynamics within these sectors could provide valuable insights for investors, policymakers, and businesses seeking to navigate and capitalize on market opportunities.

### Stacked Bar Plot for each year and constituent sector in S&P500 Index with the total traded Volume
```{r}
ggplot(newdf, aes(x = year(newdf$Date), y = Volume, fill = GICS)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

<p>The bar plot illustrates the trading volume trends for different GICS sectors in the S&P 500 index from 2000 to 2024. Overall, the plot reveals fluctuations in trading activity over the years, with some sectors consistently experiencing higher trading volumes than others. Major historical events may have influenced trading patterns, as evidenced by spikes or dips in volume around these events. The analysis provides valuable insights into sector-specific trading behaviors and their contributions to overall market activity, enhancing our understanding of market dynamics and informing investment strategies.


# SMART Question 1:  Are there any specific industries that drive the index?

### Identifying Impact of various sectors on closing price of the index

```{r, include=TRUE, results=T}
#checking which industries drive the index
# Fit a linear regression model
model <- lm(Close ~ GICS, data = df)

# Perform an ANOVA test
anova_result <- anova(model)

# Print the ANOVA table
print(anova_result)

# Alternatively, you can also examine the coefficients
summary(model)
```
### Plot of the bars with respect to sectors estimated values for Closing price

```{r, include=TRUE,results=T}
# Create a data frame with coefficients and sectors
coefficients <- coef(model)[-1]  # Exclude intercept
sectors <- names(coefficients)
data <- data.frame(sectors, coefficients)

# Plot
ggplot(data, aes(x = sectors, y = coefficients)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = round(coefficients, 2)), vjust = -0.5) +
  labs(title = "Coefficients of GICS Sectors on Close Price",
       x = "GICS Sectors",
       y = "Coefficient") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


<p>The ANOVA table for the "Close" response variable indicates that the GICS sector significantly influences the closing price of the stock overall (p < 2.2e-16). The coefficients table provides the estimated impact of each GICS sector on the closing price relative to the baseline category. For example, the Consumer Discretionary sector is expected to have an impact 83.821 units greater than the baseline on the closing price, while the Energy sector's anticipated impact is -5.661 units lower compared to the baseline.

<p>The residuals analysis shows that the model has a minimum residual of -140, a maximum residual of 7960, and a relatively symmetric distribution with a median of -28 and a mean close to zero. This indicates that the model adequately captures the variation in the closing price.

<p>Overall, the ANOVA and coefficients tables, along with the residuals analysis, provide valuable insights into the relationship between the GICS sectors and the closing price of the stock, aiding investors and analysts in understanding the factors influencing stock prices and making informed decisions.

### Identifying Impact of various sectors on Volume of stocks traded from that sectors

```{r, include=TRUE, results=T}
#checking which industries drive the index
# Fit a linear regression model
model1 <- lm(Volume ~ GICS, data = df)

# Perform an ANOVA test
anova_result <- anova(model1)

# Print the ANOVA table
print(anova_result)

# Alternatively, you can also examine the coefficients
summary(model1)
```
### Plot of the bars with respect to sectors estimated values for Volume

```{r, include=TRUE,results=T}
# Create a data frame with coefficients and sectors
coefficients1 <- coef(model1)[-1]  # Exclude intercept
sectors1 <- names(coefficients1)
data1 <- data.frame(sectors1, coefficients1)

# Plot
ggplot(data1, aes(x = sectors1, y = coefficients1)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = round(coefficients1, 2)), vjust = -0.5, color = "black", size = 3) +  # Add labels
  labs(title = "Coefficients of GICS Sectors on Volume Traded",
       x = "GICS Sectors",
       y = "Coefficient") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

<p>From the plot and the coefficients table, it is evident that each GICS sector significantly influences the volume traded, as indicated by the low p-values (<2e-16) for all sectors. The sectors with the most substantial estimated impact on volume traded are Consumer Staples, Industrials, and Financials, with coefficients of -15171682, -17153271, and -14706167, respectively. These sectors are expected to have a considerable negative impact on the volume traded compared to the baseline.

<p>In contrast, the Information Technology sector has a relatively smaller impact on volume traded, with a coefficient of -4483219. This indicates that compared to the baseline, the Information Technology sector is expected to have a smaller negative impact on volume traded.

<p>Overall, the plot and coefficients table provide valuable insights into how different GICS sectors influence the volume traded, helping investors and analysts understand the market dynamics and make informed decisions.

```{r, include=TRUE, results=T}
total_volume <- df %>%
  group_by(GICS) %>%
  summarise(total_volume = sum(Volume))

# Bar plot of total volume by GICS sector
ggplot(total_volume, aes(x = GICS, y = total_volume, fill = GICS)) +
  geom_bar(stat = "identity") +
  labs(title = "Total Volume by GICS Sector", x = "GICS Sector", y = "Total Volume") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

+ Here as we can see IT has the highest volume of stocks being traded in the time period, followed by Consumer Discretionary.


# SMART Question 2: How does different GICS Sectors respond to major historical Events

### Defining a function to calculate average returns for each company and then average returns for each GICS sector including the average close price for each company

```{r}
# Function to calculate returns and average close price for each company and then average returns for each GICS sector
sector_returns <- function(data, start_date, end_date) {
  # Calculate returns for each company and average close price
  company_returns_close <- data %>%
    group_by(Symbol) %>%
    filter(Date >= start_date & Date <= end_date) %>%
    reframe(start_close = Close[Date == start_date],
            end_close = Close[Date == end_date],
            avg_close = mean(Close)) %>%
    mutate(return = (end_close - start_close) / start_close * 100) %>%
    select(Symbol, return, avg_close)
  
  # Calculate average returns and average close price for each GICS sector
  sector_returns_close <- data %>%
    inner_join(company_returns_close, by = "Symbol") %>%
    group_by(GICS) %>%
    summarize(avg_return = mean(return), avg_close = mean(avg_close))
  
  return(sector_returns_close)
}

# Calculate sector returns and average close price for each event
recession_returns_close <- sector_returns(df_recession, "2007-12-31", "2009-06-30")
covid_returns_close <- sector_returns(df_covid, "2020-02-03", "2023-05-11")
ukraine_returns_close <- sector_returns(df_ukraine, "2022-02-24", max(df_ukraine$Date))

head(recession_returns_close, 2)
head(covid_returns_close, 2)
head(ukraine_returns_close, 2)
```

Subsetting the data for 1 Year before the start of event and after the end of events for T-Tests

```{r}
year_before_recession <- df[df$Date >= "2006-12-29" & df$Date <= "2007-12-31", ]
year_after_recession <- df[df$Date >= "2009-06-30" & df$Date <= "2010-06-30", ]
year_before_covid <- df[df$Date >= "2019-02-01" & df$Date <= "2020-02-03", ]
# using lockdown end date as the end date for covid
year_after_covid <- df[df$Date >= "2020-08-31" & df$Date <= "2021-08-31", ]
year_before_ukraine <- df[df$Date >= "2021-02-24" & df$Date <= "2022-02-23", ]
year_after_ukraine <- df[df$Date >= "2023-02-24" & df$Date <= "2024-02-24", ]
```

Calculating the average returns for each sector for 1 year before and after the event

```{r}
# Calculate sector returns for each event and year before and after the event using function defined above
recession_returns_before <- sector_returns(year_before_recession, "2006-12-29", "2007-12-31")
recession_returns_after <- sector_returns(year_after_recession, "2009-06-30", "2010-06-30")
covid_returns_before <- sector_returns(year_before_covid, "2019-02-01", "2020-02-03")
covid_returns_after <- sector_returns(year_after_covid, "2020-08-31", "2021-08-31")
ukraine_returns_before <- sector_returns(year_before_ukraine, "2021-02-24", "2022-02-23")
ukraine_returns_after <- sector_returns(year_after_ukraine, "2022-02-24", "2023-02-24")
```

Create a function to conduct t-tests for each sector before and after the events
```{r}
sector_ttest <- function(sector, before_data, after_data) {
  subset_before <- before_data %>%
    filter(GICS == sector) %>%
    group_by(Date) %>%
    summarize(avg_close = mean(Close))
  
  subset_after <- after_data %>%
    filter(GICS == sector) %>%
    group_by(Date) %>%
    summarize(avg_close = mean(Close))
  
  ttest_result <- t.test(subset_before$avg_close, subset_after$avg_close)
  
  return(list(before_data = subset_before,
              after_data = subset_after,
              ttest_result = ttest_result))
}
```

### Conduct t-test for "Industrials", "Information Technology", "Financials", "Health Care" sector before and after the Great Recession, Covid-19 Pandemic, and Russia-Ukraine Invasion

```{r}
industrials_recession <- sector_ttest("Industrials", year_before_recession, year_after_recession)
info_tech_recession <- sector_ttest("Information Technology", year_before_recession, year_after_recession)
financials_recession <- sector_ttest("Financials", year_before_recession, year_after_recession)
health_care_recession <- sector_ttest("Health Care", year_before_recession, year_after_recession)

# Covid-19 Pandemic
industrials_covid <- sector_ttest("Industrials", year_before_covid, year_after_covid)
info_tech_covid <- sector_ttest("Information Technology", year_before_covid, year_after_covid)
financials_covid <- sector_ttest("Financials", year_before_covid, year_after_covid)
health_care_covid <- sector_ttest("Health Care", year_before_covid, year_after_covid)

# Russia-Ukraine Invasion
industrials_ukraine <- sector_ttest("Industrials", year_before_ukraine, year_after_ukraine)
info_tech_ukraine <- sector_ttest("Information Technology", year_before_ukraine, year_after_ukraine)
financials_ukraine <- sector_ttest("Financials", year_before_ukraine, year_after_ukraine)
health_care_ukraine <- sector_ttest("Health Care", year_before_ukraine, year_after_ukraine)


# Printing results of the t-tests for the Great Recession
industrials_recession$ttest_result
info_tech_recession$ttest_result
financials_recession$ttest_result
health_care_recession$ttest_result

# Printing results of the t-tests for the Covid-19 Pandemic
industrials_covid$ttest_result
info_tech_covid$ttest_result
financials_covid$ttest_result
health_care_covid$ttest_result

# Printing results of the t-tests for the Russia-Ukraine Invasion
industrials_ukraine$ttest_result
info_tech_ukraine$ttest_result
financials_ukraine$ttest_result
health_care_ukraine$ttest_result
```

### Visualization of the sector returns during historic events

```{r}
# Create a function to plot sector returns
plot_sector_returns <- function(returns, title, color) {
  ggplot(returns, aes(x = GICS, y = avg_return)) +
    geom_bar(stat = "identity", fill = color) +
    labs(x = "GICS Sector", y = "Average Return (%)", title = title) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

# Plot sector returns for each event
plot_sector_returns(recession_returns_close, "Sector Returns during the Great Recession", "steelblue")
plot_sector_returns(covid_returns_close, "Sector Returns during the Covid-19 Pandemic", "orange")
plot_sector_returns(ukraine_returns_close, "Sector Returns during the Russia-Ukraine Invasion", "lightgreen")
```

### Interpretation of the sector returns during historic events
<p>During the Great Recession, the real estate, energy, and financial sectors were among the hardest hit. The real estate industry suffered from a housing market collapse, leading to a significant decline in property values and investments. The financial sector faced a crisis due to the collapse of major financial institutions and a credit crunch, resulting in severe economic downturns.

<p>The Covid-19 Pandemic brought contrasting impacts on different industries. The energy and information technology sectors experienced substantial growth. The energy sector saw increased demand for renewable energy sources and innovations in energy technology. The information technology sector benefited from the accelerated digital transformation driven by remote work and online services. However, the real estate and utilities sectors faced challenges, with real estate experiencing declines in property sales and rentals, and utilities experiencing disruptions in demand and supply chains.

<p>The invasion of Russia and Ukraine provided the information technology industry with another opportunity to showcase its resilience and growth. The sector expanded steadily, leveraging advancements in technology and digital solutions. The energy and industrials sectors also experienced significant growth during this period, driven by increased demand for energy and industrial products amidst geopolitical tensions.

<p>Throughout these events, the real estate industry consistently performed poorly, facing challenges such as declining property values, reduced investments, and disruptions in construction and development projects. The industry's performance underscores its vulnerability to economic shocks and its slower recovery compared to other sectors.

# SMART Question 3: Can we develop a predictive model to forecast the S&P500 index?

## ARIMA Model for Forecasting

### Data Loading
```{r, include=F}
df <- data.frame(read.csv('https://raw.githubusercontent.com/DATS6101-TeamNeo/datasets/main/dataset2003-2023.csv'))
df$Date <- as.Date(df$Date)
str(df)
work_df <- subset(df, select = -c(6))
str(work_df)
summary(work_df)
```
```{r, include=T, results=T}
work_df$daily_change <- (work_df$Close / lag(work_df$Close) - 1) * 100
head(work_df)
work_df <- na.omit(work_df)
head(work_df)
```

### Check if the time series is Stationary.
```{r, include=T, results=T}
adf.test(work_df$Close)
# Check the stationarity using the ACF and PACF plots.
acf(work_df$Close, main = "ACF Plot")
pacf(work_df$Close, main = "PACF Plot")
```

<p>To address the non-stationarity of the data, we are applying first-order differencing. This process involves calculating the difference between each observation and the previous observation in the dataset. By doing so, we aim to remove any trend or seasonality present in the data, making it more stationary and suitable for certain types of analyses, such as time series modeling.

<p>First, we identify the column in the dataset that we want to difference. In this case, let's assume the column is named 'column_name'. We use the diff function in R to calculate the first-order differences and store the results in a new column called 'diff_column'. The differences = 1 argument specifies that we are taking the difference between consecutive observations.

<p>After applying differencing, we can visualize the stationary series using a line plot. The x-axis represents the date or time period, while the y-axis shows the differenced values. This plot helps us assess the effectiveness of differencing in removing trends or patterns from the data, making it more suitable for further analysis.

### Making the time series data stationary

Using first order differencing to make the time series data stationary.
```{r, include=T, results=T}
ts_data <- xts(work_df$Close, order.by = work_df$Date)
```

### ADF test for the original time series data.

```{r, include=T, results=T}
adf.test(ts_data)
diff_ts_data <- diff(ts_data, differences = 1)
diff_ts_data <- na.omit(diff_ts_data)
```

### ADF test for the differenced time series data.

```{r, include=T, results=T}
adf.test(diff_ts_data)
```

```{r, include=T, results=T}
acf(diff_ts_data, main = "ACF Plot")
pacf(diff_ts_data, main = "PACF Plot")
```

## ARIMA model fitting and forecasting

<p>After conducting the Augmented Dickey-Fuller (ADF) test and analyzing the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots, we can infer that the time series data is close to stationarity. The ADF test helps us determine the presence of a unit root in the data, which is indicative of non-stationarity. A p-value greater than the significance level (usually 0.05) suggests that we cannot reject the null hypothesis of a unit root, indicating non-stationarity. Conversely, a p-value below the significance level indicates stationarity.

<p>The ACF plot shows the autocorrelation of the time series with its lagged values. A sharp drop-off in autocorrelation after a few lags suggests that the data is close to stationary. The PACF plot helps us identify the direct effect of past observations on the current value, which is useful for determining the order of the ARIMA model.

<p>Based on these analyses, we can proceed to create a baseline ARIMA (AutoRegressive Integrated Moving Average) model for forecasting future values. ARIMA models are commonly used for time series forecasting and are characterized by three components: autoregression (AR), differencing (I), and moving average (MA). The baseline model serves as a starting point for further refinement and model selection.
```{r, include=T, results=T}
train_data <- head(diff_ts_data, n = round(0.98 * length(diff_ts_data)))
test_data <- diff_ts_data[-seq_along(train_data)]
arima_model <- auto.arima(train_data)
summary(arima_model)
forecasts <- forecast(arima_model, h = length(test_data))
```

### Evaluation of the ARIMA model

```{r, include=T, results=T}
# Convert the forecasts to the original scale.
last_train_date <- index(tail(train_data, 1))
last_observed_value <- ts_data[last_train_date]
actual_preds <- as.vector(last_observed_value) + cumsum(forecasts$mean)
actual_test <- as.vector(last_observed_value) + cumsum(test_data)
actual_preds <- xts(actual_preds, order.by = index(actual_test))
# Compute the forecasting errors
errors <- actual_preds - actual_test
# Calculate evaluation metrics
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
```

<p>The Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are common metrics used to evaluate the performance of forecasting models. In this case, the MAE is `mae`, the MSE is `mse`, and the RMSE is `rmse`. These metrics provide insights into the accuracy and precision of the ARIMA model in predicting the S&P500 index.

<p>The ARIMA model's performance, as indicated by these scores, is not optimal for forecasting the S&P500 index. This is expected because the ARIMA model relies solely on previous `Close` values to make predictions, without considering other contextual factors that could influence stock market movements, such as economic indicators, company performance, or market sentiment.

<p>To potentially improve the forecasting accuracy, we can explore other models like Long Short-Term Memory (LSTM), which is a type of recurrent neural network (RNN) well-suited for sequence prediction tasks like time series forecasting. LSTM models can capture long-term dependencies in sequential data, making them potentially more effective in capturing the complex patterns present in stock market data compared to traditional ARIMA models.

<p>By implementing an LSTM model and comparing its performance with the ARIMA model, we can determine if the LSTM model provides better results in forecasting the S&P500 index.

### Plots

#### Plot train and test data.
```{r, include=T, results=T}
plot(ts_data[index(ts_data) <= last_train_date], type = "l", xlab = "Time", ylab = "Index closing price", main = "Train Data")
plot(ts_data[index(ts_data) > last_train_date], type = "l", xlab = "Time", ylab = "Index closing price", main = "Test Data")
```

### Plot the forecasted data along with the train and test data.

```{r, include=T, results=T}
plot_start_date <- as.Date("2023-01-01")
actual_train <- ts_data[index(ts_data) > plot_start_date & index(ts_data) <= last_train_date]
# Convert the time series data to data frames
train_df <- data.frame(Date = index(actual_train), Value = coredata(actual_train), Type = "Train data")
test_df <- data.frame(Date = index(actual_test), Value = coredata(actual_test), Type = "Test data")
forecast_df <- data.frame(Date = index(actual_preds)[-1], Value = actual_preds[-1], Type = "Forecast")
# Combine the data frames
combined_df <- rbind(train_df, test_df, forecast_df)
# Create the plot
ggplot(combined_df, aes(x = Date, y = Value, color = Type, linetype = Type)) +
  geom_line() +
  labs(title = "Train, Test, and Forecast Data Plot",
       x = "Date",
       y = "Closing Price") +
  scale_color_manual(values = c("red", "orange", "blue")) +
  scale_linetype_manual(values = c("dashed", "solid", "solid")) +
  theme_minimal() +
  theme(legend.position = "right")
```

```{r, include=T, results=T}
# Linear Regression.

# Assuming 'work_df' contains your preprocessed dataset
# You may need to select appropriate features and split the data into train and test sets

work_df <- data.frame(subset(df, select = c(1, 5)))
# Feature selection (Example: using lagged values as features)
for (i in 1:7) {
  work_df[[paste0('lag', i)]] <- lag(work_df$Close, i)
}
work_df <- na.omit(work_df)

# Split data into train and test sets
train_size <- 0.98
train_index <- round(nrow(work_df) * train_size)
train_data <- work_df[1:train_index, ]
test_data <- work_df[(train_index + 1):nrow(work_df), ]

# Train the linear regression model
lm_model <- lm(Close ~ lag1 + lag2 + lag3 + lag4 + lag5 + lag6 + lag7, data = train_data)

future_data <- test_data

future_data[, 3:ncol(future_data)] <- 0
future_data[1, 3:ncol(future_data)] <- rev(tail(train_data$Close, 7))

# Make predictions on test data
for (i in 1:nrow(future_data)) {
  future_data$Close[i] <- predict(lm_model, newdata = future_data[i, ])
  if (i < nrow(future_data)){
    future_data[i + 1, 3:ncol(future_data)] <- c(future_data$Close[i], future_data[i, 3:(ncol(future_data) - 1)])
  }
}
predictions <- future_data$Close

# Evaluate the model
errors <- predictions - test_data$Close
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)

# Print evaluation metrics
print(paste("Mean Absolute Error (MAE):", mae))
print(paste("Mean Squared Error (MSE):", mse))
print(paste("Root Mean Squared Error (RMSE):", rmse))
```


```{r, include=T, results=T}
train_df <- data.frame(Date = work_df[1:train_index, "Date"], Value = coredata(work_df[1:train_index, "Close"]), Type = "Train data")
test_df <- data.frame(Date = work_df[(train_index+1):nrow(work_df), "Date"], Value = coredata(work_df[(train_index+1):nrow(work_df), "Close"]), Type = "Test data")
forecast_df <- data.frame(Date = work_df[(train_index+1):nrow(work_df), "Date"], Value = coredata(predictions), Type = "Forecast")

# Combine the data frames
combined_df <- rbind(train_df[train_df$Date > plot_start_date, ], test_df, forecast_df)

# Create the plot
ggplot(combined_df, aes(x = Date, y = Value, color = Type, linetype = Type)) +
  geom_line() +
  labs(title = "Train, Test, and Forecast Data Plot",
       x = "Date",
       y = "Closing Price") +
  scale_color_manual(values = c("red", "orange", "blue")) +
  scale_linetype_manual(values = c("dashed", "solid", "solid")) +
  theme_minimal() +
  theme(legend.position = "right")
```

```{r, include=T, results=T}
ggplot(combined_df, aes(x = Date, y = Value, color = Type, linetype = Type)) +
  geom_line() +
  labs(title = "Train, Test, and Forecast Data Plot",
       x = "Date",
       y = "Closing Price") +
  scale_color_manual(values = c("red", "orange", "blue")) +
  scale_linetype_manual(values = c("solid", "solid", "solid")) +
  theme_minimal() +
  theme(legend.position = "right")
```

# LSTM Model for Forecasting

```{r, include=F}
df <- data.frame(read.csv("https://raw.githubusercontent.com/DATS6101-TeamNeo/datasets/main/dataset2003-2023.csv"))
str(df)
summary(df)
```

```{r, include=T, results=T}
work_df <- subset(df, select = -(6))
work_df$daily_change <- (work_df$Close - lag(work_df$Close)) / lag(work_df$Close) * 100
head(work_df)
work_df <- na.omit(work_df)
head(work_df)
```

```{r, echo=FALSE}
work_df <- data.frame(subset(df, select = c(1, 5)))
# Feature selection (Example: using lagged values as features)
for (i in 1:7) {
  work_df[[paste0('lag', i)]] <- lag(work_df$Close, i)
}
work_df <- na.omit(work_df)

# Split data into train and test sets
train_size <- 0.98
train_index <- round(nrow(work_df) * train_size)
train_data <- work_df[1:train_index, ]
test_data <- work_df[(train_index + 1):nrow(work_df), ]
```

```{r, include=T, results=T}
library(keras)
library(tensorflow)

model <- keras_model_sequential() %>%
  layer_lstm(units = 2048, activation="relu", return_sequences = TRUE, input_shape = c(7, 1)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_lstm(units = 1024, activation="relu", return_sequences = TRUE) %>%
  layer_dropout(rate = 0.2) %>%
  layer_lstm(units = 512, activation="relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)
```

<p>Installing TensorFlow in RStudio requires several steps to ensure compatibility and functionality. The process involves installing the reticulate, tensorflow, and keras packages, which enable RStudio to interface with TensorFlow, a popular machine learning framework.

<p>First, install the reticulate package to enable RStudio to use Python libraries. Use the command install.packages("reticulate") in the RStudio console.

<p>Next, install the tensorflow package, which provides the TensorFlow API for R. Use the command install.packages("tensorflow") in the RStudio console.

<p>Finally, install the keras package, which provides a high-level neural networks API. Use the command install.packages("keras") in the RStudio console.

<p>Following these steps ensures that TensorFlow is successfully installed in RStudio, allowing for seamless integration of TensorFlow's powerful machine learning capabilities into R projects.

```{r, include=T, results=T}
model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = 'adam'
)
```

```{r, include=T, results=T}
x_train <- as.list(train_data[, 3:9])
x_train <- do.call(cbind, x_train)
y_train <- as.list(train_data[, 2])
y_train <- do.call(rbind, y_train)

x_val <- as.list(test_data[, 3:9])
x_val <- do.call(cbind, x_val)
y_val <- as.list(test_data[, 2])
y_val <- do.call(rbind, y_val)
```

```{r, include=T, results=T}
# Uncomment to run training
#model %>% fit(
#  x_train, y_train,
#  epochs = 10,
#  batch_size = 32,
#  validation_data = list(x_val, y_val),
#  callbacks = list(
#  callback_model_checkpoint("checkpoints.h5", save_best_only = TRUE)
#  )
#)
```

```{r, include=T, results=T}

model <- load_model_hdf5("checkpoints.h5")

summary(model)

# Compute the forecasting errors
predictions <- model %>% predict(x_val)
errors <- predictions - test_data$Close

# Calculate evaluation metrics
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)

#print(paste("Mean Absolute Error (MAE):", mae))
#print(paste("Mean Squared Error (MSE):", mse))
#print(paste("Root Mean Squared Error (RMSE):", rmse))
```

<p>In our analysis, we evaluated the performance of the forecasting model using three key metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics provide valuable insights into the accuracy and precision of the model's predictions.

<p>The MAE, which is `r mae`, represents the average magnitude of the errors in the predictions, providing a measure of the model's accuracy. The MSE, which is `r mse`, measures the average of the squares of the errors, giving a sense of the variance in the errors. Finally, the RMSE, which is `r rmse`, is the square root of the MSE and provides an easily interpretable estimate of the average error magnitude.

<p>Our analysis indicates that the forecasting model, based on these metrics, does not perform optimally in predicting the S&P500 index. These results suggest that the model may benefit from further refinement or the exploration of alternative modeling approaches to improve forecasting accuracy.


```{r, include=T, results=T}

train_ts <- xts(do.call(rbind, as.list(train_data$Close)), order.by = as.Date(train_data$Date))
test_ts <- xts(test_data$Close, order.by = as.Date(test_data$Date))
forecast_ts <- xts(predictions, order.by = as.Date(test_data$Date))
```

```{r, include=T, results=F}

plot_train_start_date <- as.Date("2023-01-01")

train_ts <- train_ts[index(train_ts) > plot_train_start_date & index(train_ts) <= tail(train_data, 1)$Date]

train_df <- data.frame(Date = index(train_ts), Value = coredata(train_ts), Type = "Train data")
test_df <- data.frame(Date = index(test_ts), Value = coredata(test_ts), Type = "Test data")
forecast_df <- data.frame(Date = index(forecast_ts)[-1], Value = coredata(forecast_ts)[-1], Type = "Forecast")

# Combine the data frames
combined_df <- rbind(train_df, test_df, forecast_df)

# Create the plot
ggplot(combined_df, aes(x = Date, y = Value, color = Type, linetype = Type)) +
  geom_line()
  labs(title = "ARIMA Model Evaluation (Train, Test, and Forecast Data Plot)",
       x = "Date",
       y = "Closing Price") +
  scale_color_manual(values = c("red", "orange", "blue")) +
  scale_linetype_manual(values = c("solid", "solid", "solid")) +
  theme_minimal() +
  theme(legend.position = "right")
```

<p>In our comparative analysis of forecasting models for the S&P500 index, we found that the Long Short-Term Memory (LSTM) model outperforms the AutoRegressive Integrated Moving Average (ARIMA) model based on key metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). The LSTM model's superior performance can be attributed to its ability to capture complex sequential patterns in the data, which are not effectively captured by the ARIMA model.

<p>The LSTM model's advantage in capturing sequential dependencies comes at the cost of longer run times and a more complex model architecture. However, for users prioritizing forecasting accuracy over simplicity, especially those intending to base investment decisions solely on the index's closing price, the LSTM model emerges as a favorable choice.

<p>Our findings suggest that the LSTM model offers a promising approach for forecasting the S&P500 index, particularly for users seeking more accurate predictions and willing to accept the trade-offs of increased computational complexity and longer training times.
